{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "\n",
    "Now we have to predict the \"future sales\". This is a process that we are going to run every business day, in the middle of the night to try to predict how much each reseller is going to buy on their next pruchase. \n",
    "\n",
    "We will be basing on the max date of the dataset + 1 day because our extraction is not updated, but in production we can use the current day of the system and trust that we have all the relevant sales history from our transactional system. \n",
    "\n",
    "Note that to compute the features, now we only need the previous 30 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import os \n",
    "import time\n",
    "from sagemaker.predictor import csv_serializer,RealTimePredictor\n",
    "import datetime\n",
    "import pickle\n",
    "import awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = max_date - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['date'] > min_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill with amount 0 all the missing sales for each reseller every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeItem(dfItem,max_date,min_date):\n",
    "    r = pd.date_range(start=min_date, end=max_date)\n",
    "    dfItemNew = dfItem.set_index('date').reindex(r).fillna(0.0).rename_axis('date').reset_index()\n",
    "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\n",
    "    return dfItemNew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCompletedList = []\n",
    "for nid,item in df.groupby('id_reseller'):\n",
    "    dfCompletedList.append(completeItem(item,max_date,min_date))\n",
    "dfCompleted = pd.concat(dfCompletedList).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfCompleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfCompleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for each reseller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_info(group):\n",
    "    weekday = (max_date + pd.Timedelta(days=1)).weekday_name\n",
    "    mean_last_30 = group['bill'].replace(0,np.nan).mean()\n",
    "    std_last_30 = group['bill'].replace(0,np.nan).std()\n",
    "    date_last_bill = group[group['bill'] != 0]['date'].max()\n",
    "    days_without_purchase = (max_date + pd.Timedelta(days=1) - date_last_bill).days\n",
    "    \n",
    "    mean_last_7 = group[(group['date'] >= max_date - pd.Timedelta(days=6))]['bill'].replace(0,np.nan).mean()\n",
    "    last_bill = group[group['bill'] > 0].sort_values('date',ascending=False).head(1)['bill'].values[0]\n",
    "    return {'weekday':weekday,'mean-last-30':mean_last_30,\n",
    "           'std-last-30':std_last_30,'mean-last-7':mean_last_7,'last_bill':last_bill, \n",
    "           'id_reseller':int(group['id_reseller'].max()), 'days_without_purchase':days_without_purchase}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for index,group in df.groupby('id_reseller'):\n",
    "    features.append(complete_info(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with reseller info and compute dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.merge(df_r,how='inner',on='id_reseller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['zone'] = df_features['zone'].apply(lambda x: x if x in [1019,1050,1031,1033,1051,1067] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"preprocessing.pkl\",\"rb\")\n",
    "pipe_list = pickle.load(pickle_in)\n",
    "# [le_cluster,ohe_cluster,le_zone,ohe_zone,le_weekday,ohe_weekday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.DataFrame(\n",
    "    pipe_list[1].transform(pipe_list[0].transform(df_features['cluster']).reshape(-1, 1)).todense()\n",
    ")\n",
    "df_cluster = df_cluster.add_prefix('cluster_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = pd.DataFrame(\n",
    "    pipe_list[3].transform(pipe_list[2].transform(df_features['zone']).reshape(-1, 1)).todense()\n",
    ")\n",
    "df_zone = df_zone.add_prefix('zone_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekday = pd.DataFrame(\n",
    "    pipe_list[5].transform(pipe_list[4].transform(df_features['weekday']).reshape(-1, 1)).todense()\n",
    ")\n",
    "df_weekday = df_weekday.add_prefix('weekday_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict = pd.concat([df_features,df_cluster,df_zone,df_weekday],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-order features\n",
    " Now we have to make sure that the features are in the same order we used for training and that we don't have any extra columns.\n",
    "\n",
    "\n",
    "### Here you are going to need the same columns and order that it's displayed in notebook PROD1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pred_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict_feats = df_to_predict[pred_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict_feats.to_csv('to_predict.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict[['id_reseller']].to_csv('id_reseller_to_predict.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
