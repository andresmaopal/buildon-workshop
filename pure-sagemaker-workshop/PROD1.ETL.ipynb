{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL\n",
    "\n",
    "In this notebook we will transform the raw data that we explored in the previous one. The target of our problem, for each day, is the amount of the next bill that the reseller will pay. \n",
    "\n",
    "As the model is going to need to run every day to keep predictions relevant, the ETL that we build here is going to be deployed in Glue as a process that we are going to run every day orchestrated by Step Functions.\n",
    "\n",
    "The Glue process is going to read the raw data from the Data Lake, and is going to take the last 4 months of history to build features relevant to predict how much is each reseller going to expend next time, based on reseller's characteristics and past shopping patterns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import pickle\n",
    "import io\n",
    "import awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'XXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = awswrangler.Session()\n",
    "df = session.pandas.read_sql_athena(\n",
    "    sql=\"select * from resellers_sample\",\n",
    "    database=\"implementationdb\"\n",
    ")\n",
    "df_r = session.pandas.read_sql_athena(\n",
    "    sql=\"select * from reseller\",\n",
    "    database=\"implementationdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the last 4 months of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = max_date - pd.to_timedelta(120, unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] > min_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engeneering based on past history\n",
    "\n",
    "For each one of the arround 12K resellers, we are going to compute features based on their shopping history: mean and standard deviation of the last week and the last month, and also the amount of the last purchase and how many days as he or she spent without shopping to the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeItem(dfItem):\n",
    "    min_date = dfItem['date'].min()\n",
    "    max_date = dfItem['date'].max()\n",
    "    if min_date == max_date:\n",
    "        #only one data point\n",
    "        return\n",
    "    r = pd.date_range(start=min_date, end=max_date)\n",
    "    dfItemNew = dfItem.set_index('date').reindex(r).rename_axis('date').reset_index()\n",
    "    \n",
    "    dfItemNew['mean-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).mean().reset_index()['bill']\n",
    "    dfItemNew['mean-last-7'] = dfItemNew['bill'].rolling(7,min_periods=1).mean().reset_index()['bill']\n",
    "    dfItemNew['std-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).std().reset_index()['bill']\n",
    "    dfItemNew['bill'] = dfItemNew['bill'].fillna(0)\n",
    "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\n",
    "    dfItemNew['std-last-30'].fillna(method='ffill',inplace=True)\n",
    "    dfItemNew['mean-last-7'].fillna(method='ffill',inplace=True)\n",
    "    resp = []\n",
    "    counter = 0\n",
    "    for index,row in dfItemNew.iterrows(): \n",
    "        resp.append(counter)\n",
    "        if row['bill'] == 0: \n",
    "            counter += 1 \n",
    "        else:\n",
    "            counter = 0\n",
    "    dfItemNew['days_without_purchase'] = pd.Series(resp)\n",
    "    return dfItemNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dfCompletedList = []\n",
    "for nid,item in df.groupby('id_reseller'):\n",
    "    i = i+1\n",
    "    if i%200 == 0:\n",
    "        print ('processed {} resellers'.format(str(i)))\n",
    "    dfCompletedList.append(completeItem(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfCompletedList).copy()\n",
    "del dfCompletedList\n",
    "df['weekday']  = df['date'].dt.weekday_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute next bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['next_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute last bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='ffill').copy()\n",
    "different_zero = df['last_bill'].shift(1)\n",
    "df.loc[df['bill'] != 0,'last_bill'] = np.nan\n",
    "df['last_bill'] = df['last_bill'].fillna(different_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the sales with the rest of the reseller info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_r,how='inner',on='id_reseller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with categorical variables\n",
    "\n",
    "To deal with categorical variables (reseller's cluster and reseller's zone), we will use a combination of sklearn's Label Encoder, a preprocessing module that transforms strings in numeric lables, and One Hot Encoder, that takes this numerical variables and creates dummy (0/1 state) variables. \n",
    "\n",
    "This modules are python objects that keep in their internal variables the information necessary to transform new data.  So, in the Glue ETL we are going to store this objects in pkl format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zone'].value_counts().head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zone'] = df['zone'].apply(lambda x: x if x in [1019,1050,1031,1033,1051,1067] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_cluster = LabelEncoder()\n",
    "ohe_cluster = OneHotEncoder(handle_unknown='ignore')\n",
    "df_cluster = pd.DataFrame(ohe_cluster.fit_transform(le_cluster.fit_transform(df['cluster'].fillna('')).reshape(-1, 1)).todense())\n",
    "df_cluster = df_cluster.add_prefix('cluster_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_zone = LabelEncoder()\n",
    "ohe_zone = OneHotEncoder(handle_unknown='ignore')\n",
    "df_zone = pd.DataFrame(ohe_zone.fit_transform(le_zone.fit_transform(df['zone'].fillna('')).reshape(-1, 1)).todense())\n",
    "df_zone = df_zone.add_prefix('zone_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_weekday = LabelEncoder()\n",
    "ohe_weekday = OneHotEncoder(handle_unknown='ignore')\n",
    "df_weekday = pd.DataFrame(ohe_weekday.fit_transform(le_weekday.fit_transform(df['weekday']).reshape(-1, 1)).todense())\n",
    "df_weekday = df_weekday.add_prefix('weekday_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')\n",
    "client.put_object(Body=pickle.dumps(le_cluster), Bucket=bucket, Key='preprocessing/le_cluster.pkl');\n",
    "client.put_object(Body=pickle.dumps(ohe_cluster), Bucket=bucket, Key='preprocessing/ohe_cluster.pkl')\n",
    "client.put_object(Body=pickle.dumps(le_zone), Bucket=bucket, Key='preprocessing/le_zone.pkl')\n",
    "client.put_object(Body=pickle.dumps(ohe_zone), Bucket=bucket, Key='preprocessing/ohe_zone.pkl')\n",
    "client.put_object(Body=pickle.dumps(le_weekday), Bucket=bucket, Key='preprocessing/le_weekday.pkl')\n",
    "client.put_object(Body=pickle.dumps(ohe_weekday), Bucket=bucket, Key='preprocessing/ohe_weekday.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to S3 resulting ETL\n",
    "\n",
    "Now we have to write to S3 all the relevant columns. We will perform a train/validation split of the customers so we can train on a group and get relevant metrics on the other.\n",
    "\n",
    "The <b>label</b> that we have to predict in this problem is the amount of the next bill. \n",
    "SageMaker requires, that the input information in csv format contains <b> no header and no index, and that the first column must be occupied by the label. </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['next_bill', 'bill', 'date', 'id_reseller', 'mean-last-30', 'mean-last-7',\n",
    "       'std-last-30', 'days_without_purchase', 'weekday', \n",
    "       'last_bill', 'zone', 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_cluster,df_zone,df_weekday],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a random 10% sample of the resellers and then shuffle the records \n",
    "val_resellers = list(pd.Series(df['id_reseller'].unique()).sample(frac=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[~df['id_reseller'].isin(val_resellers)].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df[df['id_reseller'].isin(val_resellers)].sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to be useful, we have to drop all the columns that are wither too specific or that are not going to be available on prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.to_csv('validation.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local pickle  preprocessing\n",
    "\n",
    "Now we pickle the sklearn's preprocessing blocks. Later we are going to use them as part of the pipeline, to transform new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_list = [le_cluster,ohe_cluster,le_zone,ohe_zone,le_weekday,ohe_weekday]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessing.pkl', 'wb') as handle:\n",
    "    pickle.dump(preprocessing_list, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
